# SPO Benchmark â€“ Simulated Parity Ontology

**SPO Benchmark** is a symbolic reasoning test suite designed to evaluate language models (LLMs) across 105 levels of increasing complexity.  
It simulates progressive symbolic challenges, from simple binary logic to ontological paradoxes, symbolic self-negation, and recursive theory of mind.

---

## ğŸ§  Purpose

To rigorously compare models like **GPT-4, Claude, Mistral, Gemini, LLaMA, SAGE-2**, and **SAGE-3** across increasingly complex symbolic reasoning tasks.

---

## ğŸ§ª How It Works

Each SPO level is a function `f(x)` that takes a binary sequence `x` and returns:

- `1` (accept) if the symbolic condition is satisfied  
- `0` (reject) otherwise

You can use it to assess:

- ğŸ§© **Symbolic abstraction capacity**  
- ğŸ•°ï¸ **Understanding of temporal and causal relations**  
- ğŸ” **Resistance to paradoxes and inconsistencies**  
- ğŸ§  **Abductive reasoning and simulated Theory of Mind**

---

## ğŸ§© Level Examples

| Level Range | Description |
|-------------|-------------|
| 1â€“10        | Basic parity and logical operations |
| 11â€“20       | Crossed window parity |
| 21â€“30       | Symbolic mirroring and consistency |
| 31â€“40       | Agent simulation with symbolic memory |
| 41â€“50       | Temporal context and dual consistency |
| 51â€“60       | Abductive cause inference |
| 61â€“70       | Recursive Theory of Mind |
| 71â€“80       | Temporal paradoxes and self-deception |
| 81â€“90       | Cross-ontological loops and contradiction |
| 91â€“105      | Symbolic self-negation, meta-parity, observer paradox, and ontological discontinuity |

---

## ğŸš€ How to Use

```python
from spo import generate_spo_levels

spo_levels = generate_spo_levels()
sample_input = [1, 0, 1, 1, 0, 0, 1]

# Run all 105 SPO levels
for i, level_fn in enumerate(spo_levels, start=1):
    output = level_fn(sample_input)
    print(f"Level {i:03d} â†’ Output: {output}")
